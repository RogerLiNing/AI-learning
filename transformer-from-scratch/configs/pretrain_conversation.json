{
  "tokenizer_path": "data/tokenizer/tokenizer_dialogue.json",
  "train_data_path": "data/processed/dialogue_conversation_train.txt",
  "val_data_path": "data/processed/dialogue_conversation_val.txt",
  "save_dir": "saved_models/pretrain_conversation",
  "log_dir": "logs/pretrain_conversation",
  
  "d_model": 512,
  "n_layers": 6,
  "n_heads": 8,
  "d_ff": 2048,
  "dropout": 0.1,
  "max_seq_len": 512,
  
  "batch_size": 32,
  "num_epochs": 10,
  "learning_rate": 5e-5,
  "weight_decay": 0.01,
  "warmup_ratio": 0.1,
  "gradient_accumulation_steps": 1,
  
  "logging_steps": 100,
  "save_steps": 1000,
  "eval_steps": 500,
  
  "num_workers": 4
}
