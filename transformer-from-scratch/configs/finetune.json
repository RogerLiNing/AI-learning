{
    "tokenizer_dir": "data/hf_tokenizer",
    "d_model": 512,
    "n_layers": 6,
    "n_heads": 8,
    "d_ff": 2048,
    "dropout": 0.1,
    "max_seq_len": 512,
    "batch_size": 8,
    "num_epochs": 5,
    "learning_rate": 2e-5,
    "weight_decay": 0.01,
    "warmup_ratio": 0.1,
    "gradient_accumulation_steps": 1,
    "logging_steps": 10,
    "save_steps": 50,
    "eval_steps": 50,
    "save_dir": "saved_models/finetuned"
}
